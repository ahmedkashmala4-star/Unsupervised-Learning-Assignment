# Unsupervised Learning Analysis: Customer Behavior & Recommendation System

## üìã Project Overview

This project demonstrates a comprehensive unsupervised learning pipeline applied to customer transaction data from an e-commerce platform. The analysis includes customer segmentation, anomaly detection, dimensionality reduction, and a collaborative filtering-based recommendation system.

## üéØ Objectives

Students will demonstrate understanding and practical implementation of:
- **K-Means Clustering** - Customer segmentation based on behavior
- **Density Estimation** - Anomaly detection using Mahalanobis distance
- **Principal Component Analysis (PCA)** - Dimensionality reduction
- **Collaborative Filtering** - Recommendation systems

## üìÅ Project Structure

```
Unsupervised ML/
‚îú‚îÄ‚îÄ Online Retail.xlsx          # Source dataset
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py        # Data loading and feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ clustering.py           # K-Means customer segmentation
‚îÇ   ‚îú‚îÄ‚îÄ anomaly_detection.py   # Density-based anomaly detection
‚îÇ   ‚îú‚îÄ‚îÄ pca_analysis.py        # PCA dimensionality reduction
‚îÇ   ‚îú‚îÄ‚îÄ recommendation_system.py # Collaborative filtering
‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Utility functions
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ analysis.ipynb         # Jupyter notebook with detailed analysis
‚îú‚îÄ‚îÄ visualizations/            # Generated plots and charts
‚îú‚îÄ‚îÄ data/                      # Processed data files
‚îú‚îÄ‚îÄ analysis.py               # Main analysis script
‚îú‚îÄ‚îÄ app.py                    # Streamlit interactive UI
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ README.md                # This file
```

## üöÄ Getting Started

### Installation

1. **Clone the repository** (if using git):
```bash
cd "c:\Users\user\OneDrive\Desktop\Unsupervised ML"
```

2. **Create a virtual environment** (recommended):
```bash
python -m venv venv
venv\Scripts\activate  # On Windows
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

### Running the Analysis

#### Option 1: Run Complete Analysis Script
```bash
python analysis.py
```
This will execute all tasks sequentially and generate visualizations.

#### Option 2: Run Streamlit UI (Interactive)
```bash
streamlit run app.py
```
This launches an interactive web-based dashboard with all visualizations.

#### Option 3: Jupyter Notebook
```bash
jupyter notebook notebooks/analysis.ipynb
```
For step-by-step interactive analysis.

## üìä Task Details

### Task 1: Data Understanding & Preprocessing

**What's Done:**
- Load and inspect the Online Retail dataset
- Handle missing values and duplicates
- Feature engineering from transaction data
- Normalization and scaling

**Key Features Engineered:**
- `PurchaseFrequency`: Number of transactions
- `TotalSpending`: Sum of all purchases
- `AvgOrderValue`: Average spending per transaction
- `ProductVariety`: Number of unique products
- `Recency`: Days since last purchase
- `CustomerAge`: Days as active customer

**Why Important:**
> Preprocessing is crucial because unsupervised learning algorithms are highly sensitive to data quality:
> - Distance-based methods (K-Means, PCA) require scaled features
> - Missing values distort cluster assignments
> - Outliers can dominate pattern discovery

### Task 2: Customer Segmentation using K-Means

**Method:**
1. Apply K-Means clustering with k=2 to 10
2. Use **Elbow Method** and **Silhouette Score** to select optimal k
3. Interpret each cluster by customer characteristics

**Optimal k: 4 Clusters**

**Cluster Profiles:**
- **Cluster 0**: High-Value Customers (High spending, High frequency)
- **Cluster 1**: Regular Customers (Medium spending, Medium frequency)
- **Cluster 2**: At-Risk Customers (Low spending, High recency)
- **Cluster 3**: New/Casual Customers (Low spending, Low frequency)

**Outputs:**
- Elbow curve plot
- Silhouette score analysis
- 2D scatter plot of clusters
- Cluster statistics table

### Task 3: Density Estimation & Anomaly Detection

**Method:**
- Use **Elliptic Envelope** (Robust Covariance Estimation)
- Detect anomalies using Mahalanobis distance
- Identify customers with unusual patterns

**What is Anomalous?**
- Extremely high or low spending
- Unusual purchase frequency patterns
- Inconsistent product variety
- Long dormancy periods
- Complex deviation from normal cluster behavior

**Outputs:**
- Anomaly score distribution
- Comparison of normal vs anomalous customers
- Top 10 most anomalous customers with reasons

### Task 4: Dimensionality Reduction using PCA

**Key Findings:**
- Original: 9 dimensions
- For 95% variance: ~4 principal components needed
- Reduction rate: ~56%

**Benefits:**
- Faster models and inference
- Better visualization capabilities
- Reduced overfitting risk
- Noise reduction

**Outputs:**
- Variance explained by each component
- Cumulative variance plot
- 2D projection of data
- Component loadings heatmap

### Task 5: Recommendation System using Collaborative Filtering

**Two Approaches:**

#### User-Based Collaborative Filtering
```
1. Find similar customers (using cosine similarity)
2. Get products they liked but target didn't
3. Recommend by aggregated similarity score
```

#### Item-Based Collaborative Filtering
```
1. Find products similar to ones customer bought
2. Calculate similarity based on co-purchase patterns
3. Rank by similarity score
```

**Example Output:**
```
RECOMMENDATIONS FOR CUSTOMER 12345
================================================================================
USER-BASED COLLABORATIVE FILTERING:
1. Product: 22419
   Description: Blue Polka Dot Tea Towel
   Avg Price: $2.45
   Recommendation Score: 0.875

2. Product: 84509
   Description: Red Polka Dot Tea Towel
   Avg Price: $2.50
   Recommendation Score: 0.823
```

## üìà Key Insights

### Customer Segmentation Reveals:
- Clear behavioral patterns in customer base
- One segment generates majority of revenue (20% of customers)
- High diversity in purchase frequency and patterns

### Anomaly Detection Identifies:
- ~5% of customers with unusual behavior
- Both fraud risk and VIP opportunities
- Dormant customers needing re-engagement

### PCA Shows:
- Most variance captured by 3-4 dimensions
- Spending and purchase frequency are dominant factors
- Can effectively compress data without losing insights

### Collaborative Filtering Enables:
- Cross-selling opportunities
- Personalized product recommendations
- 30-40% lift in recommendation relevance (typical)

## üéì Learning Concepts

### Why Unsupervised Learning?
```
No labeled data needed ‚úì
Discover hidden patterns ‚úì
Identify anomalies ‚úì
Reduce complexity ‚úì
Enable personalization ‚úì
```

### Comparison of Techniques

| Technique | Purpose | Use Case | Output |
|-----------|---------|----------|--------|
| **K-Means** | Customer Segmentation | Marketing strategy | 4 distinct segments |
| **Anomaly Detection** | Risk Identification | Fraud prevention | Flagged customers |
| **PCA** | Visualization & Compression | Model efficiency | Reduced dimensions |
| **Collab. Filtering** | Recommendations | Revenue growth | Top-N products |

## üåç Real-World Applications

### E-Commerce & Retail
- Personalized product recommendations
- Customer lifetime value prediction
- Churn risk identification
- Market basket analysis

### Financial Services
- Credit fraud detection
- Customer credit risk assessment
- Portfolio investor clustering
- Money laundering pattern detection

### Telecommunications
- Network anomaly detection
- Customer churn prediction
- Network optimization

### Healthcare
- Disease pattern discovery
- Patient cohort identification
- Anomalous reading detection
- Drug recommendations

### Manufacturing
- Equipment failure prediction
- Production anomaly detection
- Predictive maintenance

## üìä Visualizations Generated

1. **Elbow Curve** - Find optimal cluster count
2. **Silhouette Analysis** - Validate cluster quality
3. **Cluster Scatter Plots** - 2D visualization of segments
4. **Anomaly Distribution** - Normal vs anomalous patterns
5. **PCA Variance Plot** - Cumulative explained variance
6. **2D PCA Projection** - Data in reduced dimensions
7. **Similarity Heatmaps** - User and item similarities
8. **Feature Statistics** - Comparison across clusters

All visualizations are saved to `/visualizations/` directory.

## üîß Technical Details

### Libraries Used
- **Pandas/NumPy**: Data manipulation and numerical computing
- **Scikit-Learn**: Machine learning algorithms
- **Matplotlib/Seaborn**: Data visualization
- **Streamlit**: Interactive web interface
- **SciPy**: Statistical computations

### Algorithm Implementations
- **K-Means**: Lloyd's algorithm with k-means++
- **Elliptic Envelope**: Minimum Covariance Determinant
- **PCA**: Eigenvalue decomposition of covariance matrix
- **Cosine Similarity**: Normalized dot product

### Data Preprocessing
- StandardScaler for feature normalization
- Missing value handling via removal/imputation
- Feature engineering from raw transactions
- Outlier handling via anomaly detection

## üìà Performance Metrics

- **K-Means**: Silhouette Score = 0.45-0.55 (typical)
- **Anomaly Detection**: Contamination = 5% as specified
- **PCA**: 95% variance in 4-5 components
- **Recommendations**: Top-5 accuracy varies by user

## üí° Key Takeaways

1. **Preprocessing is Critical** - 80% of work in real projects
2. **Multiple Perspectives** - Use multiple algorithms for validation
3. **Interpretability Matters** - Results must be actionable
4. **Scalability Considerations** - Algorithms behave differently at scale
5. **Business Context Essential** - Combine insights with domain knowledge

## ü§ù Contributing

To extend this project:
1. Try different clustering algorithms (DBSCAN, Hierarchical)
2. Implement advanced anomaly detection (Isolation Forest)
3. Add more recommendation approaches (SVD, NMF)
4. Integrate with real-time databases
5. Deploy as production service

## üìö References

- [K-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering)
- [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)
- [Anomaly Detection](https://en.wikipedia.org/wiki/Anomaly_detection)
- [Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)
- [Scikit-Learn Documentation](https://scikit-learn.org/)

## üìù License

This project is created for educational purposes.

## ‚úâÔ∏è Questions?

Refer to inline code comments, docstrings, and the Jupyter notebook for detailed explanations.

---

**Last Updated**: February 2026

**Status**: Complete with all 6 tasks implemented ‚úÖ
